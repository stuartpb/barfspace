# Arch Kubic and Image Builder / Etcher / Node Admin / Infrastructure

I think I get Kubic well enough now to remake it on Arch Linux ("Kardinal"?):

UPDATE: this project is now called "Ska Linux" (Snapshotted Kubernetes Arch) - split and scope-culling will be brought to this shortly

- [rudeboy](a05d194a-4e24-4c02-b443-14ec00a5987f.md)
- [Ska Linux Is](cf99237c-d20c-457a-a562-0159aed838cb.md)

## Core Assumption

So, one of the big defaults of this is to present the user with "Look, we're gonna be claiming ownership of the 192.168.69.0/24 for this system"

I guess the subnet mask thing might be an issue? Need to think about this

Anyway, the point of this is to sidestep the need for the existing setup (which might be a cheap home router) to host the DHCP or DNS in a way that works for us

But of course anything between "I have a shitty $12 clearance router" and "I have a $1000 OpenWRT behemoth" should be adaptable as well (ie. "I have to use my lab's DHCP, but they have no DNS")

## Etcher-like app

You'd have some kind of kiosk that can build and flash an image with everything set up and preconfigured - forget Ignition sticks, that's for chumps.

This would also track the local config in a file / push it out to the live cluster

Like, yeah, ideally this can, essentially, do kubeadm init before flashing the image

## Transactional Updates

https://github.com/openSUSE/transactional-update/blob/master/sbin/transactional-update.in

I'm thinking this could even be written in Lua, to save me some pain

Does this have the config for "restart policy"? Like, setting the sentinel would definitely have to be a configurable feature for this

## dracut

It appears transactional updates has a dracut component: dracut would probably be a thing the Arch Distro would install and use? need to look closer at what it all does exactly

## "Node Configuration Operator"

This would be an operator that looks for a document that would describe, essentially, all the things the "imager app" would handle when creating a new node, and reconfiguring live nodes to match:

- Kubelet configuration?
  - Like, advanced user? Go ahead, tweak kubelet!
  - Roll out updates even maybe!
  - Still trying to understand the DNS situation?
- Package and update management
  - Additional packages
  - Freeze / holds
  - Package removals from "core" (replaced with alternatives)

[Dynamic Kubelet Config](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/) is also a thing I ought to look into

## Automatic Updates

This is another thing that runs on the host, and as such should be managed by systemd: however, there might as well be a daemon that manages the Service file(s)

I guess that'd be the part that would handle the trigger file (it adds it as a PostExec to the service?)

## systemd-networkd and systemd-resolved

I think these can cover for a lot of the shortcomings of other solutions in the space, and a lot of the problems that would hence go unsolved

- Switching to wireless, but only when Ethernet isn't available
- A search domain that can only be accessed on the host network

This is very much a "for people who can run on metal" - but if you can run outside that, you're using the base image (or preconfigured-for-VMs) anyway

I guess VMs would probably need some variant on this design with a different set of Network configurations

The stuff with Domains= and search domains is the stuff that dovetails nicely with the "Node Name Resolution DNS Server" thing below

[systemd-resolved and Kubernetes](3fcc4b71-cddd-4ae2-a8ec-c8937368c29c.md)

### Wireless

Just a reminder you need to make wpa_supplicant configurable

### Side thought re: systemd-networkd's definition capabilities

You could use this to make a physically-isolated cluster network, where only the control nodes / API servers / load-balancers are bridged to the Internet

You'd need a few USB network adapters, of course

## Cluster DHCP / Node Name Resolution DNS Server

All the nodes can ship pre-configured to look for a specific DHCP server, or to use a server for DNS, for names in a cluster node name zone

You can also do it so each node is preconfigured with its own IP - this actually seems the most straightforward, the way this is being designed

Anyway, the best way to handle node DNS would be to ship them out as part of the image routing to a fixed IP address

from the definition of the nodes in the cluster, from the Kubernetes API, pushed like External DNS to a configured nsd (and unbound maybe?)

anyway, this DNS server can then be configured as the search path for names under the cluster node namespace (something like `nodes.cluster.local` by default)

### more on systemd-networkd capabilities

https://jlk.fjfi.cvut.cz/arch/manpages/man/systemd.network.5

remember, DHCPServer to run a DHCP Server for an interface is an option

damn, you can even set explicit ARP neighbors!

### what about mDNS et al?

idk, they seem really complicated, I haven't quite grasped how they'd fit into my existing setup, and I can figure out more clearly how this would work without them.

Is it less cooperative than one of those solutions might be? Maybe. Is there anything to be lost in being so?

## CRI-O et al

All the pod-oriented container infrastructure that Kubic uses - the stuff that interested me in it in the first place - is smooth and cleanly-factored for use in this, moreso than Docker.

## Node Setup

Like, you should be able to pre-configure all a node's metadata, set all its labels, etc- not just pick a hostname and (automatically/randomly by default) an IP

There's all this stuff that's been mentioned with the host environment, and then there's stuff that's a subset of the things that go on the Node document in Kubernetes. I think the abstract node should be the base document, and then all this stuff like the host stuff would be further levels of abstracted document

Though, really, I don't want to go too far down the rabbit hole with this: really, the Transactional System on the nodes themselves is capable of handling most of these kinds of changes? And the daemons can just be in rolling it all out over SSH? idk, food for thought

## "Fun" setup

Fun stuff like "pick another name out of X bag randomly, for this tagged set I'm setting up"

this is also where all the brand thoughts come in

## Pre-joining images?

What if you never had to SSH in to kubeadm token create or cpy-paste a link or any of that - you just flash the image, put in the card, and the node immediately connects to the host, it's pre-authorized, just like if you'd plugged it in and run `kubeadm join` and unplugged it

## Maybe make this an OS image, and the "native app" is a VM

This actually seems like it's probably the safest approach: run the whole thing in a kiosk in a VM, and then you could ship a laptop with the "image pre-heater" OS as native

Also insulates from the host distro...

How much of this would still be doable as an AppImage? Electron?

You could then also use it as a dashboard for the nodes

this starts to resemble Arges and the [Sturling Arges](4eb71c89-f1d5-4d98-8363-1eb9e5cc0ca1.md) idea

## x86?

I'm thinking this should be made for x64s that can boot from SD cards. Like, really, if you want to image a disk that's not SD with this, you can - but don't play like that's going to be some big hassle. I mean, come on, this setup kiosk I'm designing here is basically the installer CD

oh yeah, that was another thought, this is a lot like the YAST installer - just primarily geared toward image building

something like this could even theoretically also be used for building docker images? idk, parts?

## Coverage vs. the Common

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/ details what distro packages are like right now
