# The First Multi-Node Stubernetes Rollout Comes to a Screeching Halt

## rolling out dashboard and rook-ceph

Thinking this might be the thing keeping prometheus down

```
  Warning  FailedReleaseSync  43s (x2 over 98s)  helm-operator  synchronization of release 'rook-ceph' in namespace 'rook-ceph' failed: installation failed: template: rook-ceph/templates/clusterrole.yaml:284:7: executing "rook-ceph/templates/clusterrole.yaml" at <((.Values.agent) and .Values.agent.mountSecurityMode) and ne .Values.agent.mountSecurityMode "Any">: can't give argument to non-function (.Values.agent) and .Values.agent.mountSecurityMode
```

looks like the fix for this landed [a week ago](https://github.com/rook/rook/pull/5660)

## after setting up openwrt

They're having DNS problems?

Oh, durr, because they're still trying to resolve at 192.168.0.1, and OpenWRT is at 192.168.1.1

rebooting both, doing `curl https://github.com/stuartpb.keys > /root/.ssh/authorized_keys` on studtop because I'm sick of finding the Ignition key

ugh, they're showing unqualified transient hostnames again. I'll figure this out later: doing `hostnamectl set-hostname ` to the FQDNs and restarting Kubelets until then

## ugh, the values workaround did not fix this

https://github.com/rook/rook/pull/5660#issuecomment-653540762

## next-level workaround

I delete the hr from stubernetes-system and just go ahead and deploy the chart manually from a checkout of the rook repo (changing the VERSION in values.yaml)

```
The Rook Operator has been installed. Check its status by running:
  kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator"

Visit https://rook.io/docs/rook/master for instructions on how to create and configure Rook clusters

Note: You cannot just create a CephCluster resource, you need to also create a namespace and
install suitable RBAC roles and role bindings for the cluster. The Rook Operator will not do
this for you. Sample CephCluster manifest templates that include RBAC resources are available:

- https://rook.github.io/docs/rook/master/ceph-quickstart.html
- https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/cluster.yaml

Important Notes:
- The links above are for the unreleased master version, if you deploy a different release you must find matching manifests.
- You must customise the 'CephCluster' resource at the bottom of the sample manifests to met your situation.
- Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the cluster.
- The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.
- The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.
- Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).
- In the 'CephCluster' you must refer to disk devices by their '/dev/something' name, e.g. 'sdb' or 'xvde'.
```

## so that's progressing

The Prometheus pod is failing, not because Prometheus itself isn't ARM-compatible, but because the init containers aren't:

```
Name:         prometheus-pop-prometheus-0
Namespace:    prometheus
Priority:     0
Node:         studrang.nodes.403.stuartpb.com/192.168.86.29
Start Time:   Fri, 03 Jul 2020 15:25:36 -0700
Labels:       app=prometheus
              controller-revision-hash=prometheus-pop-prometheus-9b4f65f64
              prometheus=pop-prometheus
              statefulset.kubernetes.io/pod-name=prometheus-pop-prometheus-0
Annotations:  <none>
Status:       Running
IP:           10.32.0.9
IPs:
  IP:           10.32.0.9
Controlled By:  StatefulSet/prometheus-pop-prometheus
Containers:
  prometheus:
    Container ID:  cri-o://f4c712e9d3e45fd45bca27d7045fba2394d182e484ded769888834060cb7df40
    Image:         quay.io/prometheus/prometheus:v2.18.1
    Image ID:      quay.io/prometheus/prometheus@sha256:33cd2bbeea5c52add05be111fdccd54d289aa609f9059b22e9476865fd22b5b5
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --web.console.templates=/etc/prometheus/consoles
      --web.console.libraries=/etc/prometheus/console_libraries
      --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      --storage.tsdb.path=/prometheus
      --storage.tsdb.retention.time=10d
      --web.enable-lifecycle
      --storage.tsdb.no-lockfile
      --web.external-url=http://pop-prometheus.prometheus:9090
      --web.route-prefix=/
    State:       Waiting
      Reason:    CrashLoopBackOff
    Last State:  Terminated
      Reason:    Error
      Message:   b msg="WAL segment loaded" segment=22 maxSegment=24
level=info ts=2020-07-04T09:12:11.742Z caller=head.go:624 component=tsdb msg="WAL segment loaded" segment=23 maxSegment=24
level=info ts=2020-07-04T09:12:11.743Z caller=head.go:624 component=tsdb msg="WAL segment loaded" segment=24 maxSegment=24
level=info ts=2020-07-04T09:12:11.743Z caller=head.go:627 component=tsdb msg="WAL replay completed" duration=28.18978ms
level=info ts=2020-07-04T09:12:11.746Z caller=main.go:694 fs_type=9123683e
level=info ts=2020-07-04T09:12:11.746Z caller=main.go:695 msg="TSDB started"
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:799 msg="Loading configuration file" filename=/etc/prometheus/config_out/prometheus.env.yaml
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:547 msg="Stopping scrape discovery manager..."
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:561 msg="Stopping notify discovery manager..."
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:543 msg="Scrape discovery manager stopped"
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:583 msg="Stopping scrape manager..."
level=info ts=2020-07-04T09:12:11.747Z caller=manager.go:882 component="rule manager" msg="Stopping rule manager..."
level=info ts=2020-07-04T09:12:11.747Z caller=manager.go:892 component="rule manager" msg="Rule manager stopped"
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:557 msg="Notify discovery manager stopped"
level=info ts=2020-07-04T09:12:11.747Z caller=main.go:577 msg="Scrape manager stopped"
level=info ts=2020-07-04T09:12:11.752Z caller=notifier.go:601 component=notifier msg="Stopping notification manager..."
level=info ts=2020-07-04T09:12:11.752Z caller=main.go:749 msg="Notifier manager stopped"
level=error ts=2020-07-04T09:12:11.753Z caller=main.go:758 err="error loading config from \"/etc/prometheus/config_out/prometheus.env.yaml\": couldn't load configuration (--config.file=\"/etc/prometheus/config_out/prometheus.env.yaml\"): open /etc/prometheus/config_out/prometheus.env.yaml: no such file or directory"

      Exit Code:    1
      Started:      Sat, 04 Jul 2020 02:12:11 -0700
      Finished:     Sat, 04 Jul 2020 02:12:11 -0700
    Ready:          False
    Restart Count:  24
    Liveness:       http-get http://:web/-/healthy delay=0s timeout=3s period=5s #success=1 #failure=6
    Readiness:      http-get http://:web/-/ready delay=0s timeout=3s period=5s #success=1 #failure=120
    Environment:    <none>
    Mounts:
      /etc/prometheus/certs from tls-assets (ro)
      /etc/prometheus/config_out from config-out (ro)
      /etc/prometheus/rules/prometheus-pop-prometheus-rulefiles-0 from prometheus-pop-prometheus-rulefiles-0 (rw)
      /prometheus from prometheus-pop-prometheus-db (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from pop-prometheus-token-vqzng (ro)
  prometheus-config-reloader:
    Container ID:  cri-o://62d1a3905485f9c45cd59b4ebecd66218780c62652d4c9ce55ff98502365658d
    Image:         quay.io/coreos/prometheus-config-reloader:v0.38.1
    Image ID:      quay.io/coreos/prometheus-config-reloader@sha256:d1cce64093d4a850d28726ec3e48403124808f76567b5bd7b26e4416300996a7
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/prometheus-config-reloader
    Args:
      --log-format=logfmt
      --reload-url=http://127.0.0.1:9090/-/reload
      --config-file=/etc/prometheus/config/prometheus.yaml.gz
      --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
    State:       Waiting
      Reason:    CrashLoopBackOff
    Last State:  Terminated
      Reason:    Error
      Message:   standard_init_linux.go:211: exec user process caused "exec format error"

      Exit Code:    1
      Started:      Sat, 04 Jul 2020 02:12:12 -0700
      Finished:     Sat, 04 Jul 2020 02:12:12 -0700
    Ready:          False
    Restart Count:  24
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:     100m
      memory:  25Mi
    Environment:
      POD_NAME:  prometheus-pop-prometheus-0 (v1:metadata.name)
    Mounts:
      /etc/prometheus/config from config (rw)
      /etc/prometheus/config_out from config-out (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from pop-prometheus-token-vqzng (ro)
  rules-configmap-reloader:
    Container ID:  cri-o://29de69f7bc11379f25cb6399f30c5d4666773b2e4fda010f6561f7e28c450dc5
    Image:         quay.io/coreos/configmap-reload:v0.0.1
    Image ID:      quay.io/coreos/configmap-reload@sha256:e2fd60ff0ae4500a75b80ebaa30e0e7deba9ad107833e8ca53f0047c42c5a057
    Port:          <none>
    Host Port:     <none>
    Args:
      --webhook-url=http://127.0.0.1:9090/-/reload
      --volume-dir=/etc/prometheus/rules/prometheus-pop-prometheus-rulefiles-0
    State:       Waiting
      Reason:    CrashLoopBackOff
    Last State:  Terminated
      Reason:    Error
      Message:   standard_init_linux.go:211: exec user process caused "exec format error"

      Exit Code:    1
      Started:      Sat, 04 Jul 2020 02:12:14 -0700
      Finished:     Sat, 04 Jul 2020 02:12:14 -0700
    Ready:          False
    Restart Count:  24
    Limits:
      cpu:     100m
      memory:  25Mi
    Requests:
      cpu:        100m
      memory:     25Mi
    Environment:  <none>
    Mounts:
      /etc/prometheus/rules/prometheus-pop-prometheus-rulefiles-0 from prometheus-pop-prometheus-rulefiles-0 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from pop-prometheus-token-vqzng (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-pop-prometheus
    Optional:    false
  tls-assets:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-pop-prometheus-tls-assets
    Optional:    false
  config-out:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  prometheus-pop-prometheus-rulefiles-0:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-pop-prometheus-rulefiles-0
    Optional:  false
  prometheus-pop-prometheus-db:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  pop-prometheus-token-vqzng:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  pop-prometheus-token-vqzng
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason   Age                    From                                      Message
  ----     ------   ----                   ----                                      -------
  Warning  BackOff  4m46s (x307 over 64m)  kubelet, studrang.nodes.403.stuartpb.com  Back-off restarting failed container
```
