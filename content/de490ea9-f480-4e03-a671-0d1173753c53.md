# Stubernetes Plan for 2020-05-13

address.exposed is too complicated for now

I'm gonna make my goal for the day to make it so I can have some kind of container with a local volume for, like, SSHing into from the pocketchip, fuck I don't know

did I even do that thing with the DNS? I think this is where my head started to hurt because of k3s's shenanigans

so I guess that's kind of the thing I really want: to rip out all the k3s service definitions I can

## The New Plan

Okay, so, you know what, fuck all of k3s's service names per https://github.com/rancher/k3s/issues/1797

Like, they're planning on switching the ingress server? but they named the document `traefik`, so they're going to have to delete documents named `traefik` at some point if they want to do that.

I'm gonna propose a new issue soon that suggests switching to these with a `k3s-` prefix later, but for now, I'm going to name each component by *what it does*, so the underlying implementation can be swapped entirely by just updating the doc.

so these docs in `kube-system` would move:

- `coredns` -> `cluster-dns`
  - this also makes it less awkward to have a second CoreDNS instance for `external-dns`
- `servicelb` / `metallb` -> `load-balancer`
- `local-path-provisioner` / `longhorn` -> `persistent-volume-provisioner`
- `traefik` -> `ingress`

aborted comment from the aforementioned GH issue (I'll just post the issue when I'm ready):

> Thinking about it a little more (and doing a code search to discover that, despite #1382's closure, k3s hasn't *quite* removed `--no-deploy`), I'm planning on reworking my cluster to use documents named

The case that I'll make in said issue is that, per https://github.com/rancher/k3s/issues/817, it doesn't make sense to stick with an implementation as the name if you think you might change the implementation

## Getting ready for Longhorn

Put a new flash drive in the USB hub

SSHed in

`sudo mkfs.ext4 /dev/sdb1`

## 2020-05-15: before I do anything else

rather than get fancy and figure out the upgrade operator, for right now I'm just gonna run [the update commands](https://github.com/rancher/k3os#manual-upgrades) via ssh--

nah, wait, it says they're deprecated...

trying to visit the dashboard the hacky way I normally'd do it, by going to https://192.168.0.23:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ and copy-pasting the credentials from .kube/config into the browser login prompt, fails with a 503 after authentication...

I just straight up do `sudo reboot` from SSH to see if that'll fix it?

hmm, no, it's still not there...

looking into doing `kubectl get logs`, I notice I have to update the IP in my `.kube/config`

okay... I was gonna say "let me restart the pod", but oh, right, I just restarted the node (uptime had been 77 days)

uhh...

let's see if we can't upgrade this node, maybe that'll do something

`kubectl edit node studtop`... uhh...

okay, looking at `kubectl edit -n k3os-system plan/k3os-latest` (ganked from https://github.com/rancher/k3os/issues/415), it looks like it did update to v0.10.1 after I rebooted

uhh, doing `kubectl proxy`, then going to https://192.168.0.23:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login, works. TODO: figure out what that's doing that hooks up the endpoint

`kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')` retrieves the token I need (pulled and corrected from [this late-February log entry](8feab719-bfad-45ac-938e-3ccb9f8c9e72.md) - the version listed there pulls the token for the wrong user, not sure where I got it from)
