# Understanding Lua: Preface

Everything I'm covering in this book is free, as in "freedom" - not only is it free of charge, but Lua's permissive license means you're free to do pretty much whatever you want with it (except pretend that you made it, or hold Lua's creators legally responsible for anything you do with it). The appendix (or something like that) will contain a guide to setting up Lua for yourself and following along.

That being said, it's almost better for you to read this book *without* trying to imitate it on your computer until you've read it all the way through: I'll be explaining a lot of concepts that aren't actually visible when you program. What you see in the margins is all that is *actually* presented: what you see in the panels is a representation of what that text *means*, so you can conceptualize the abstract concepts.

I'm using Lua here as a basis to teach these programming concepts, but many of these ideas can be applied to other languages (and almost all other languages' concepts can be derived from the base present in Lua). The concepts fit JavaScript, the language of the Web, probably better than any other language in wide use today. (They fit well enough that I'll remark on the differences when they come up.)

You may have been turned off from programming in the past by having been given the impression that general-purpose programming involves a strong command of mathematics, or "computer science" or "information theory" or some other such high-minded academic expertise. While certain *kinds* of programming can require these skills (for example, it'd be hard to program a simulation of bodies in space without a solid understanding of Newtonian physics), the fundamental nature of programming involves *basically no special knowledge of any kind*.

I'll touch on how to write code that *performs* basic arithmetic (addition, subtraction, multiplication, division), as well as some slightly more high-school math (exponentiation, trigonometry), but even if you don't know what these kinds of math are or how you'd use them, it won't stop you from doing anything that doesn't fundamentally require them. (That being said, sometimes math *is* necessary: if you're writing a program to determine how many apples you'll have if you give a certain number of apples away, you'll be hard-pressed to find a good way to do it without using subtraction.)
